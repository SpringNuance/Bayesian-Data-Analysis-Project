---
title: "BDA Project: Malicious and Benign Website URL detections"
author: "Nguyen Xuan Binh, Duong Le"
date: "January 2023"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
    fig_caption: yes
bibliography: bibliography.bib
---

```{r include=FALSE}
library(rstan)
library(cmdstanr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(scales)
library(loo)
library(sentimentr)
library(stringr)
library(gridExtra)
library(MASS)
#library(fitdistrplus)

```

# Introduction
Web Security is a challenging task amidst ever rising threats on the Internet. With billions of websites active on Internet, and hackers evolving newer techniques to trap web users, machine learning offers promising techniques to detect malicious websites. The dataset described in this manuscript is meant for such machine learning based analysis of malicious and benign webpages. The data has been collected from Internet using a specialized focused web crawler named MalCrawler [1]. The dataset comprises of various extracted attributes, and also raw webpage content including JavaScript code. It supports both supervised and unsupervised learning. For supervised learning, class labels for malicious and benign webpages have been added to the dataset using the Google Safe Browsing API.1 The most relevant attributes within the scope have already been extracted and included in this dataset. However, the raw web content, including JavaScript code included in this dataset supports further attribute extraction, if so desired. Also, this raw content and code can be used as unstructured data input for text-based analytics. This dataset consists of data from approximately 1.5 million webpages, which makes it suitable for deep learning algorithms. This article also provides code snippets used for data extraction and its analysis.


## Problem Description


## Data Description

The dataset contains extracted attributes from websites that can be used for Classification of webpages as malicious or benign. The dataset also includes raw page content including JavaScript code that can be used as unstructured data in Deep Learning or for extracting further attributes. The data has been collected by crawling the Internet using MalCrawler [1]. The labels have been verified using the Google Safe Browsing API [2]. Attributes have been selected based on their relevance [3]. The details of dataset attributes is as given below: 
'url'             - The URL of the webpage.
'ip_add'      - IP Address of the webpage.
'geo_loc'     - The geographic location where the webpage is hosted.
'url_len'      - The length of URL.
'js_len'        - Length of JavaScript code on the webpage.
'js_obf_len - Length of obfuscated JavaScript code.
'tld'             - The Top Level Domain of the webpage.
'who_is'      - Whether the WHO IS domain information is compete or not.
'https'         - Whether the site uses https or http.
'content'     - The raw webpage content including JavaScript code.
'label'          - The class label for benign or malicious webpage. 
 
Python code for extraction of the above listed dataset attributes is attached.
The Visualisation of this dataset and it python code is also attached. This visualisation can be seen online on Kaggle

Kaggle: https://www.kaggle.com/datasets/aksingh2411/dataset-of-malicious-and-benign-webpages
Data source: https://data.mendeley.com/datasets/gdx3pkwp47/2

## Main modeling idea

# Data cleaning 



```{r}
train_websites_top_10 <- read.csv("websites/train_websites_top_10.csv")
test_websites_top_10 <- read.csv("websites/test_websites_top_10.csv")
train_websites_top_10 <- train_websites_top_10 
test_websites_top_10 <- test_websites_top_10
```


# Separate model

```{r}

# Get unique country names
countries <- unique(train_websites_top_10$geo_loc)

# Get number of countries
K <- length(countries)

# Number of URLs per country, varying length of vector element
N_list = list()


# For each country, saving the number of URLs
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  N_list <- c(N_list, nrow(train_websites_country))
}

M_list = list()
# For each country, saving the number of URLs
for (country in countries) {
  test_websites_country <- test_websites_top_10 %>%
    filter(geo_loc == country)
  M_list <- c(M_list, nrow(test_websites_country))
}

# The matrix of Javascript code, varying length of vector element
js_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  js_len_list <- c(js_len_list, list(train_websites_country$js_len))
}

# The matrix of obfuscated Javascript code, varying length of vector element
js_obf_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  js_obf_len_list <- c(js_obf_len_list, list(train_websites_country$js_obf_len))
}

# The matrix of safety level of the URL, varying length of vector element
safety_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  safety_list <- c(safety_list, list(train_websites_country$safety))
}

# Testing data
# The matrix of Javascript code, varying length of vector element
js_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_10 %>%
    filter(geo_loc == country)
  js_len_test_list <- c(js_len_test_list, list(test_websites_country$js_len))
}

# The matrix of obfuscated Javascript code, varying length of vector element
js_obf_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_10 %>%
    filter(geo_loc == country)
  js_obf_len_test_list <- c(js_obf_len_test_list, list(test_websites_country$js_obf_len))
}

# The matrix of safety level of the URL, varying length of vector element
safety_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_10 %>%
    filter(geo_loc == country)
  safety_test_list <- c(safety_test_list, list(test_websites_country$safety))
}

# The matrix of label of the URL (malicious/benign), varying length of vector element
label_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  label_list <- append(label_list, list(train_websites_country$label))
}

# first country: 


stan_data <- list(
  N = N_list[[1]],
  M = M_list[[1]],
  js_len = js_len_list[[1]],
  js_obf_len = js_obf_len_list[[1]],
  safety = safety_list[[1]],
  js_len_pred = js_len_test_list[[1]],
  js_obf_len_pred = js_obf_len_test_list[[1]],
  safety_pred = safety_test_list[[1]],
  label = label_list[[1]]
)


```
```{r}
print(N_list[[1]])
print(length(js_len_list[[1]]))
print(length(js_obf_len_list[[1]]))
print(length(label_list[[1]]))
```

```{r}

#print(label_list[[1]])
file_separate <- file.path("model_separate.stan")
model_separate <- cmdstan_model(file_separate)
model_separate$compile(quiet = FALSE)
result_separate <- model_separate$sample(data = stan_data, chains=4, show_messages=FALSE)
result_separate$output(1)
```

# Hierarchical model

```{r}

# Get unique country names
countries <- unique(train_websites_top_10$geo_loc)

# Get number of countries
K <- length(countries)

# Number of URLs per country, varying length of vector element
N = list()

# For each country, saving the number of URLs
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  N <- append(N, nrow(train_websites_country))
}

# The matrix of Javascript code, varying length of vector element
js_len = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  js_len <- c(js_len, list(train_websites_country$js_len))
}

# The matrix of obfuscated Javascript code, varying length of vector element
js_obf_len = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  js_obf_len <- c(js_obf_len, list(train_websites_country$js_obf_len))
}

# The matrix of safety level of the URL, varying length of vector element
safety = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  safety <- append(safety, list(train_websites_country$safety))
}

# The matrix of label of the URL (malicious/benign), varying length of vector element
label = list()
for (country in countries) {
  train_websites_country <- train_websites_top_10 %>%
    filter(geo_loc == country)
  label <- append(label, list(train_websites_country$label))
}

stan_data <- list(
  N = N,
  js_len = js_len,
  js_obf_len = js_obf_len,
  safety = safety,
  label = label
)

file_separate <- file.path("model_separate.stan")
model_separate <- cmdstan_model(file_separate)
model_separate$compile(quiet = FALSE)
result_separate <- model_separate$sample(data = stan_data, chains=4, show_messages=FALSE)
```

## Model description


## Stan code and running option


## Convergence diagnostics


## Posterior predictive check


## Predictive performance assessment


## Prior sensitivity analysis

# Hierarchical model

## Model description


## Stan code and running options


## Convergence diagnostics


## Posterior predictive check


## Predictive performance assessment

## Prior sensitivity analysis



# Model comparision



# Disscussion


# Conclusion of the results




# Reflection


# References


