---
title: "BDA Project: Malicious and Benign Website URL detections"
author: "Nguyen Xuan Binh, Duong Le"
date: "January 2023"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
    fig_caption: yes
bibliography: bibliography.bib
---

# Introduction

## Central problem
Web Security is a challenging task amidst ever rising threats on the Internet. With billions of websites active on Internet, and hackers evolving newer techniques to trap web users, machine learning offers promising techniques to detect malicious websites. The dataset described in this manuscript is meant for such machine learning based analysis of malicious and benign webpages. The data has been collected from Internet using a specialized focused web crawler named MalCrawler [1]. The dataset comprises of various extracted attributes, and also raw webpage content including JavaScript code. It supports both supervised and unsupervised learning. For supervised learning, class labels for malicious and benign webpages have been added to the dataset using the Google Safe Browsing API.1 The most relevant attributes within the scope have already been extracted and included in this dataset. However, the raw web content, including JavaScript code included in this dataset supports further attribute extraction, if so desired. Also, this raw content and code can be used as unstructured data input for text-based analytics. This dataset consists of data from approximately 1.5 million webpages, which makes it suitable for deep learning algorithms. This article also provides code snippets used for data extraction and its analysis.

## Motivation


## Main modeling idea


# Dataset 

## Data Description

The dataset contains extracted attributes from websites that can be used for Classification of webpages as malicious or benign. The dataset also includes raw page content including JavaScript code that can be used as unstructured data in Deep Learning or for extracting further attributes. The data has been collected by crawling the Internet using MalCrawler [1]. The labels have been verified using the Google Safe Browsing API [2]. Attributes have been selected based on their relevance [3]. The details of dataset attributes is as given below: 
'url'         - The URL of the webpage.
'ip_add'      - IP Address of the webpage.
'geo_loc'     - The geographic location where the webpage is hosted.
'url_len'     - The length of URL.
'js_len'      - Length of JavaScript code on the webpage.
'js_obf_len - Length of obfuscated JavaScript code.
'tld'         - The Top Level Domain of the webpage.
'who_is'      - Whether the WHO IS domain information is compete or not.
'https'         - Whether the site uses https or http.
'content'     - The raw webpage content including JavaScript code.
'label'          - The class label for benign or malicious webpage. 
 
Python code for extraction of the above listed dataset attributes is attached.
The Visualisation of this dataset and it python code is also attached. This visualisation can be seen online on Kaggle

## Data source and analysis difference
Kaggle: https://www.kaggle.com/datasets/aksingh2411/dataset-of-malicious-and-benign-webpages
Data source: https://data.mendeley.com/datasets/gdx3pkwp47/2
https://www.researchgate.net/publication/347936136_Malicious_and_Benign_Webpages_Dataset

```{r, include=FALSE}
library(rstan)
library(cmdstanr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(scales)
library(loo)
library(sentimentr)
library(stringr)
library(gridExtra)
library(MASS)
library(Metrics)
library(caret)
library(cvms)
library(tibble)
library(posterior)
library(purrr)
options(dplyr.summarise.inform = FALSE)
```


```{r}
train_websites_top_5 <- read.csv("websites/train_websites_top_5.csv")
test_websites_top_5 <- read.csv("websites/test_websites_top_5.csv")
cat("Number of training data:",nrow(train_websites_top_5))
cat("\nNumber of testing data:",nrow(test_websites_top_5))
head(train_websites_top_5)
```


# Feature selection and transformation
```{r}
train_websites <- read.csv("websites/train_websites.csv")
test_websites <- read.csv("websites/test_websites.csv")
```

```{r,echo=FALSE}
# Count the number of rows for each combination of https and who_is

train_websites_count <- train_websites %>% 
  filter(label == "good") 
numberOfMaliciousURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "good") %>% 
  group_by(https) %>%
  summarize(count = n())

# Create a pie chart with a legend
p1 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(https))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = -1) +
  scale_fill_manual(values = c("red", "green"), labels=c("no","yes")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("HTTPS in",numberOfMaliciousURLs,"benign URLs\n(yes/no)")) +
  
  guides(fill=guide_legend(title=""))

# Count the number of rows for each combination of https and who_is

train_websites_count <- train_websites %>% 
  filter(label == "good") 
numberOfBenignURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "good") %>% 
  group_by(who_is) %>%
  summarize(count = n()) %>% 
  arrange(desc(count))

# Create a pie chart with a legend
p2 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(who_is))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = 1) +
  scale_fill_manual(values = c("green", "red"), labels=c("complete","incomplete")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("WHOIS in",numberOfBenignURLs,"benign URLs\n(complete/incomplete)")) +
  
  guides(fill=guide_legend(title=""))

train_websites_count <- train_websites %>% 
  filter(label == "good") 
numberOfBenignURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "good") %>% 
  group_by(https, who_is) %>%
  summarize(count = n())

# Create a pie chart with a legend
p3 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(https, who_is))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = 1) +
  scale_fill_manual(values = c("blue", "green", "red", "orange")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("(HTTPS.WHOIS) pair combination\nin",numberOfBenignURLs,"benign URLs")) +
  guides(fill=guide_legend(title="", nrow=2))

grid.arrange(p1, p2, p3, ncol = 3)
```

```{r, echo=FALSE}
# Count the number of rows for each combination of https and who_is

train_websites_count <- train_websites %>% 
  filter(label == "bad") 
numberOfMaliciousURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "bad") %>% 
  group_by(https) %>%
  summarize(count = n())

# Create a pie chart with a legend
p1 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(https))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = 1) +
  scale_fill_manual(values = c("red", "green"), labels=c("no","yes")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("HTTPS in",numberOfMaliciousURLs,"malicious URLs\n(yes/no)")) +
  
  guides(fill=guide_legend(title=""))

# Count the number of rows for each combination of https and who_is

train_websites_count <- train_websites %>% 
  filter(label == "bad") 
numberOfMaliciousURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "bad") %>% 
  group_by(who_is) %>%
  summarize(count = n()) %>% 
  arrange(desc(count))

# Create a pie chart with a legend
p2 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(who_is))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = -1) +
  scale_fill_manual(values = c("green", "red"), labels=c("complete","incomplete")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("WHOIS in",numberOfMaliciousURLs,"malicious URLs\n(complete/incomplete)")) +
  
  guides(fill=guide_legend(title=""))

# Count the number of rows for each combination of https and who_is

train_websites_count <- train_websites %>% 
  filter(label == "bad") 
numberOfMaliciousURLs <- nrow(train_websites_count)
  
train_websites_count <- train_websites %>% 
  filter(label == "bad") %>% 
  group_by(https, who_is) %>%
  summarize(count = n())

# Create a pie chart with a legend
p3 <- ggplot(train_websites_count, aes(x = "", y = count, fill = interaction(https, who_is))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0, direction = 1) +
  scale_fill_manual(values = c("blue", "green", "red", "orange")) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 10), legend.text = element_text(size = 8)) +
  ggtitle(paste("(HTTPS.WHOIS) pair combination\nin",numberOfMaliciousURLs,"malicious URLs")) +
  guides(fill=guide_legend(title="")) +
  guides(fill=guide_legend(title="", nrow=2))

grid.arrange(p1, p2, p3, ncol = 3)
```

```{r}
ggplot(data = train_websites, aes(x = js_len, y = js_obf_len, color = label)) +
  geom_point() +
  scale_color_manual(values = c("red", "blue"), 
                     labels = c("malicious", "benign"),
                     guide = guide_legend(title = "Label")) +
  ggtitle("js_len vs js_obf_len") +
  xlab("js_len") +
  ylab("js_obf_len") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_vline(xintercept = 250, linetype = "dashed", color = "black") + 
  geom_hline(yintercept = 100, linetype = "dashed", color = "black") +
  annotate("text", x = 260, y = Inf, label = "js_len = 250", hjust = 0, vjust = 1) +
  annotate("text", x = Inf, y = 60, label = "js_ofs_len = 100", hjust = 1, vjust = 0) 
  #guides(color = guide_legend(title = "Label"))
```




```{r}
# Set up the plotting grid
par(mfrow = c(2,1))

hist(train_websites$js_len, main = "JS length histogram of the recorded URLs", xlab = "Javascript length", breaks = 100)
hist(train_websites$js_obf_len, main ="Obfuscated JS length histogram of the recorded URLs", xlab = "Obfuscated Javascript length",breaks = 100)
```

```{r}
# Group the dataframe by geo_loc and count the number of rows for each country
train_websites_count <- train_websites %>% 
  group_by(geo_loc) %>%
  summarize(count = n() ) %>%
  top_n(5, count) %>%
  slice_tail(n=5)

# Count the number of benign and malicious URLs for each country
train_websites_count_label <- train_websites %>% 
  filter(geo_loc %in% train_websites_count$geo_loc) %>%
  group_by(geo_loc, label) %>%
  summarize(count = n())

# Plot the bar chart
ggplot(train_websites_count_label, aes(x = geo_loc, y = count, fill = label)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("red", "blue"), 
                    labels = c("malicious","benign")) +
  xlab("Country") +
  ylab("Number of URLs") +
  ggtitle("Distribution of benign and malicious URLs of top 5 recorded countries") +
  guides(fill = guide_legend(title = "Label")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 0.5, size = 10, 
                                  margin = margin(r = -20, unit = "pt"),
                                  #family = "serif", 
                                  lineheight = 0.9, color = "black"))
```

# Separate model

## Model description

## Prior choice and justifications
Default protocol https is used by 81.5% of all the websites. @https://w3techs.com/technologies/details/ce-httpsdefault

There are 1.24 billion with complete WHOIS registration, while there are currently 1.7 billion websites. So the ratio of complete WHOIS website is 0.73.

## Stan code and running options

```{r, echo=FALSE}

# Get unique country names
countries <- unique(train_websites_top_5$geo_loc)

# Get number of countries
K <- length(countries)

# Number of URLs per country, varying length of vector element
N_list = list()


# For each country, saving the number of URLs
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  N_list <- c(N_list, nrow(train_websites_country))
}

M_list = list()
# For each country, saving the number of URLs
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  M_list <- c(M_list, nrow(test_websites_country))
}

# The matrix of Javascript code, varying length of vector element
js_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  js_list <- c(js_list, list(train_websites_country$js))
}

# The matrix of Javascript code, varying length of vector element
js_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  js_len_list <- c(js_len_list, list(train_websites_country$js_len_bin))
}

# The matrix of Javascript code, varying length of vector element
js_obf_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  js_obf_len_list <- c(js_obf_len_list, list(train_websites_country$js_obf_len_bin))
}

# The matrix of safety level of the URL, varying length of vector element
safety_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  safety_list <- c(safety_list, list(train_websites_country$safety))
}

# The matrix of safety level of the URL, varying length of vector element
https_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  https_list <- c(https_list, list(train_websites_country$https_bin))
}

# The matrix of safety level of the URL, varying length of vector element
whois_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  whois_list <- c(whois_list, list(train_websites_country$whois_bin))
}


# Testing data
# The matrix of Javascript code, varying length of vector element
js_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  js_test_list <- c(js_test_list, list(test_websites_country$js))
}

# The matrix of Javascript code, varying length of vector element
js_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  js_len_test_list <- c(js_len_test_list, list(test_websites_country$js_len_bin))
}

# The matrix of Javascript code, varying length of vector element
js_obf_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  js_obf_len_test_list <- c(js_obf_len_test_list, list(test_websites_country$js_obf_len_bin))
}

# The matrix of safety level of the URL, varying length of vector element
https_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  https_test_list <- c(https_test_list, list(test_websites_country$https_bin))
}

# The matrix of safety level of the URL, varying length of vector element
whois_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  whois_test_list <- c(whois_test_list, list(test_websites_country$whois_bin))
}

# The matrix of safety level of the URL, varying length of vector element
safety_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  safety_test_list <- c(safety_test_list, list(test_websites_country$safety))
}

# The matrix of label of the URL (malicious/benign), varying length of vector element
label_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  label_list <- append(label_list, list(train_websites_country$label_bin))
}

# The matrix of label of the URL (malicious/benign), varying length of vector element
label_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  label_test_list <- append(label_test_list, list(test_websites_country$label_bin))
}
```

The Stan model code:
```{r, echo = TRUE, results = 'hide'}
"
data {
  int<lower=1> N; // Number of URLs of a country (training)
  int<lower=1> M; // Number of URLs of a country (testing)
  array[N] int<lower=0,upper=1> js_len;
  array[N] int<lower=0,upper=1> js_obf_len;
  array[N] int<lower=0,upper=1> https;
  array[N] int<lower=0,upper=1> whois;

  // The testing predicting features
  array[M] int<lower=0,upper=1> js_len_pred;
  array[M] int<lower=0,upper=1> js_obf_len_pred;
  array[M] int<lower=0,upper=1> https_pred;
  array[M] int<lower=0,upper=1> whois_pred;
  // label for each URL: benign(0) or malicious(1)
  array[N] int<lower=0,upper=1> label; 
}

parameters {
  real<lower=0, upper=1> theta_js_len; // probability for js_len
  real<lower=0, upper=1> theta_js_obf_len; // probability for js_obf_len
  real<lower=0, upper=1> theta_https; // probability for https
  real<lower=0, upper=1> theta_whois; // probability for whois
  real js_len_coeff; // Slope coefficient for js
  real js_obf_len_coeff; // Slope coefficient for js
  real https_coeff; // Slope coefficient for js
  real whois_coeff; // Slope coefficient for js
  real intercept; // Intercept coefficient for js
}

model {
    // Prior for the probabilities
    theta_js_len ~ beta(1,10);
    theta_js_obf_len ~ beta(1,10);
    theta_https ~ beta(8,10);
    theta_whois ~ beta(7,10);
    // likelihood for safety and js
    js_len ~ bernoulli(theta_js_len);
    js_obf_len ~ bernoulli(theta_js_obf_len);
    https ~ bernoulli(theta_https);
    whois ~ bernoulli(theta_whois);
    // Weakly informative prior for the coefficients
    js_len_coeff ~ normal(0,20);
    js_obf_len_coeff ~ normal(0,20);
    https_coeff ~ normal(0,20);
    whois_coeff ~ normal(0,20);
    intercept ~ normal(0,20);
    // Modelling of the label based on bernoulli logistic regression by multiple variable linear regression 
    for (i in 1:N){
      label[i] ~ bernoulli(inv_logit(intercept + https_coeff * https[i] + whois_coeff * whois[i] + js_len_coeff * js_len[i] + js_obf_len_coeff * js_obf_len[i]));
    }
}

generated quantities {
    vector[N] label_train_pred;
    vector[M] label_test_pred;
    // Predictions for the training data
    for (i in 1:N){
      label_train_pred[i] = bernoulli_rng(inv_logit(intercept + https_coeff * https[i] + whois_coeff * whois[i] + js_len_coeff * js_len[i] + js_obf_len_coeff * js_obf_len[i]));
    }
    // Predictions for the testing data
    for (i in 1:M){
      label_test_pred[i] = bernoulli_rng(inv_logit(intercept + https_coeff * https_pred[i] + whois_coeff * whois_pred[i] + js_len_coeff * js_len_pred[i] + js_obf_len_coeff * js_obf_len_pred[i]));
    }
}

"
```

```{r, echo=FALSE}
# Compiling the separate Stan model
file_separate <- file.path("model_separate.stan")
model_separate <- cmdstan_model(file_separate)
model_separate$compile(quiet = FALSE)
```

The sampling running options
```{r, echo=FALSE}
separate_sampling <- list()
for (i in 1:K){
  stan_data <- list(
    N = N_list[[i]],
    M = M_list[[i]],
    js_len = js_len_list[[i]],
    js_obf_len = js_obf_len_list[[i]],
    https = https_list[[i]],
    whois = whois_list[[i]],
    js_len_pred = js_len_test_list[[i]],
    js_obf_len_pred = js_obf_len_test_list[[i]],
    https_pred = https_test_list[[i]],
    whois_pred = whois_test_list[[i]],
    label = label_list[[i]]
  )
  separate_sampling[[countries[i]]] <- model_separate$sample(data = stan_data, chains=4, iter_warmup = 2000, iter_sampling = 2000, show_messages=FALSE)#, refresh=0)
  cat("\nSampling for country",countries[i],"\n")
}
```


```{r}
# Set up the plotting grid
par(mfrow = c(3,5))

row_labels <- c("intercept", "HTTPS coefficient", "WHOIS coefficient")
row_names <- c("intercept","https_coeff","whois_coeff")

# Loop through the countries
for(i in 1:3){
    for (j in 1:5){
      # Create the subplot
      hist(separate_sampling[[countries[j]]]$draws(row_names[i]), main = countries[j], xlab=row_labels[i])
      # Add the country name to the top of the column
      mtext(countries[j], side = 3, line = 0.2, outer = TRUE)
    }
}

# Set up the plotting grid
par(mfrow = c(2,5))

row_labels <- c("JS length coefficient","JS obf. length coefficient")
row_names <- c("js_len_coeff","js_obf_len_coeff")

# Loop through the countries
for(i in 1:2){
    for (j in 1:5){
      # Create the subplot
      hist(separate_sampling[[countries[j]]]$draws(row_names[i]), main = countries[j], xlab=row_labels[i])
      # Add the country name to the top of the column
      mtext(countries[j], side = 3, line = 0.2, outer = TRUE)
    }
}
```

## Convergence diagnostics

MCMC convergence chains visualization
```{r, echo=FALSE}
plotConvergence <- function (draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  iters = length(chain1)
  indices <- 1:iters
  data <- data.frame(indices, chain1, chain2, 
                              chain3, chain4)
  
  ggplot(data, aes(x=indices)) +
    ggtitle(paste("Separate model - USA country\nFour MCMC of the",paramName,"\n",iters,"iterations, no warm-up")) +
    xlab("iteration") + 
    ylab(paramName) +
    theme(plot.title = element_text(hjust = 0.5), legend.position = "right") +
    geom_line(aes(y = chain1, color = "chain1")) + 
    geom_line(aes(y = chain2, color = "chain2")) +
    geom_line(aes(y = chain3, color = "chain3")) + 
    geom_line(aes(y = chain4, color = "chain4")) +
    scale_color_manual(name = "MCMC chains", values = c("chain1" = "red", "chain2" = "blue", "chain3" = "green", "chain4" = "black")) 
}

intercept_draws <- separate_sampling[[countries[1]]]$draws("intercept", format = "matrix")
plotConvergence(intercept_draws, "intercept")

https_draws <- separate_sampling[[countries[1]]]$draws("https_coeff", format = "matrix")
plotConvergence(https_draws, "https coefficient")

whois_draws <- separate_sampling[[countries[1]]]$draws("whois_coeff", format = "matrix")
plotConvergence(whois_draws, "https coefficient")

js_len_draws <- separate_sampling[[countries[1]]]$draws("js_len_coeff", format = "matrix")
plotConvergence(js_len_draws, "JS length coefficient")

js_obf_len_draws <- separate_sampling[[countries[1]]]$draws("js_obf_len_coeff", format = "matrix")
plotConvergence(js_obf_len_draws, "JS obfuscated length coefficient")
```

Convergence analysis using $\hat{R}$-values
```{r}
# Rhat is in assignment 5
calculate_Rhat <- function(draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  
  sims <- matrix(c(chain1, chain2, chain3, chain4), ncol = 4)
  cat("\nThe Rhat value for",paramName,"after 2000 warm-up iterations is:",rhat_basic(x=sims, split=TRUE))

}

# ESS is in assignment 4
calculate_ESS <- function(draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  
  sims <- matrix(c(chain1, chain2, chain3, chain4), ncol = 4)
  cat("\nThe Rhat value for",paramName,"after 2000 warm-up iterations is:",rhat_basic(x=sims, split=TRUE))

}

# You can also use summary() to calculate Rhat and ESS but it is kinda slow this time. We don't know why
# separate_sampling[[countries[1]]]$summary()

intercept_draws <- separate_sampling[[countries[1]]]$draws("intercept", format = "matrix")
calculate_Rhat(intercept_draws, "intercept")

https_draws <- separate_sampling[[countries[1]]]$draws("https_coeff", format = "matrix")
calculate_Rhat(https_draws, "https coefficient")

whois_draws <- separate_sampling[[countries[1]]]$draws("whois_coeff", format = "matrix")
calculate_Rhat(whois_draws, "whois coefficient")

js_len_draws <- separate_sampling[[countries[1]]]$draws("js_len_coeff", format = "matrix")
calculate_Rhat(js_len_draws, "JS length coefficient")

js_obf_len_draws <- separate_sampling[[countries[1]]]$draws("js_obf_len_coeff", format = "matrix")
calculate_Rhat(js_obf_len_draws, "JS obfuscated length coefficient")

```
HMC specific convergence diagnostics
```{r, echo=FALSE}
for (i in 1:K){
  cat("Country",countries[i],"\n\n")
  print(separate_sampling[[countries[i]]]$diagnostic_summary())
}
```
Rhat values and effective sample size
```{r}
separate_sampling[[countries[1]]]$summary(c("intercept", "https_coeff","whois_coeff","js_len_coeff","js_obf_len_coeff"))[, c("variable", "rhat", "ess_bulk", "ess_tail")]

```
## Posterior predictive checks
```{r}
metricsName <- c("Accuracy", "Prevalence", "Sensitivity", "Specificity", "Precision", "Recall", "F1")
metricsSummary <- data.frame(Metrics=metricsName)
metricsSummary$Accuracy <- NULL
metricsSummary$Prevalence <- NULL
metricsSummary$Precision <- NULL
metricsSummary$Recall <- NULL
metricsSummary$Sensitivity <- NULL
metricsSummary$Specificity <- NULL
metricsSummary$F1 <- NULL

sumTP <- 0
sumTN <- 0
sumFP <- 0
sumFN <- 0
listTrue <- c()
listPred <- c()
for (i in 1:K){
  draws <- separate_sampling[[countries[i]]]$draws("label_train_pred", format = "matrix")
  confusion_matrix <- table(as.vector(draws[8000,]), label_list[[i]])
  listTrue <- c(listTrue, label_list[[i]])
  listPred <- c(listPred, as.vector(draws[8000,]))

  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <-confusion_matrix[1,2]
  FN <-confusion_matrix[2,1]
  sumTP <- sumTP + TP
  sumTN <- sumTN + TN
  sumFP <- sumFP + FP
  sumFN <- sumFN + FN
  accuracy <- (TP+TN)/(TP+FP+FN+TN) 
  prevalence <- (TP+FN)/(TP+FP+FN+TN) 
  sensitivity <- TP/(TP+FN)
  specificity <- FN/(FN+FP)
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN)
  f1 <-  2*(precision*recall)/(precision+recall)
  metricsSummary[,countries[i]] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
}
accuracy <- (sumTP+sumTN)/(sumTP+sumFP+sumFN+sumTN) 
prevalence <- (sumTP+sumFN)/(sumTP+sumFP+sumFN+sumTN) 
sensitivity <- sumTP/(sumTP+sumFN)
specificity <- sumFN/(sumFN+sumFP)
precision <- sumTP/(sumTP+sumFP)
recall <- sumTP/(sumTP+sumFN)
f1 <-  2*(precision*recall)/(precision+recall)
metricsSummary[,"All countries"] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
metricsSummary
```

```{r, warning = FALSE}
confusion_matrix <- tibble("actual" = listTrue,
                     "prediction" = listPred)


basic_table <- table(confusion_matrix)
cfm <- as_tibble(basic_table)
plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n", palette = "Oranges")
```

## Predictive performance assessment

```{r}
metricsName <- c("Accuracy", "Prevalence", "Sensitivity", "Specificity", "Precision", "Recall", "F1")
metricsSummary <- data.frame(Metrics=metricsName)
metricsSummary$Accuracy <- NULL
metricsSummary$Prevalence <- NULL
metricsSummary$Precision <- NULL
metricsSummary$Recall <- NULL
metricsSummary$Sensitivity <- NULL
metricsSummary$Specificity <- NULL
metricsSummary$F1 <- NULL

sumTP <- 0
sumTN <- 0
sumFP <- 0
sumFN <- 0
listTrue <- c()
listPred <- c()
for (i in 1:K){
  draws <- separate_sampling[[countries[i]]]$draws("label_test_pred", format = "matrix")
  confusion_matrix <- table(as.vector(draws[8000,]), label_test_list[[i]])
  listTrue <- c(listTrue, label_test_list[[i]])
  listPred <- c(listPred, as.vector(draws[8000,]))

  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <-confusion_matrix[1,2]
  FN <-confusion_matrix[2,1]
  sumTP <- sumTP + TP
  sumTN <- sumTN + TN
  sumFP <- sumFP + FP
  sumFN <- sumFN + FN
  accuracy <- (TP+TN)/(TP+FP+FN+TN) 
  prevalence <- (TP+FN)/(TP+FP+FN+TN) 
  sensitivity <- TP/(TP+FN)
  specificity <- FN/(FN+FP)
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN)
  f1 <-  2*(precision*recall)/(precision+recall)
  metricsSummary[,countries[i]] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
}
accuracy <- (sumTP+sumTN)/(sumTP+sumFP+sumFN+sumTN) 
prevalence <- (sumTP+sumFN)/(sumTP+sumFP+sumFN+sumTN) 
sensitivity <- sumTP/(sumTP+sumFN)
specificity <- sumFN/(sumFN+sumFP)
precision <- sumTP/(sumTP+sumFP)
recall <- sumTP/(sumTP+sumFN)
f1 <-  2*(precision*recall)/(precision+recall)
metricsSummary[,"All countries"] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
metricsSummary
```

```{r, warning = FALSE}
confusion_matrix <- tibble("actual" = listTrue,
                     "prediction" = listPred)


basic_table <- table(confusion_matrix)
cfm <- as_tibble(basic_table)
plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n", palette = "Oranges")
```


## Prior sensitivity analysis









# Pooled model

## Model description

## Prior choice and justifications
Default protocol https is used by 81.5% of all the websites. @https://w3techs.com/technologies/details/ce-httpsdefault

There are 1.24 billion with complete WHOIS registration, while there are currently 1.7 billion websites. So the ratio of complete WHOIS website is 0.73.

## Stan code and running options
```{r, echo=FALSE}

# Get unique country names
countries <- unique(train_websites_top_5$geo_loc)

# Get number of countries
K <- length(countries)

# Number of URLs per country, varying length of vector element
N_list = list()

# For each country, saving the number of URLs
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  N_list <- c(N_list, nrow(train_websites_country))
}

# Maximum length of training URLs for all countries
Nmax <- N_list[[which.max(N_list)]]

M_list = list()
# For each country, saving the number of URLs
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  M_list <- c(M_list, nrow(test_websites_country))
}

# Maximum length of training URLs for all countries
Mmax <- M_list[[which.max(M_list)]]

# The matrix of Javascript code
js_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  js_len_list <- c(js_len_list, list(train_websites_country$js_len_bin))
}

js_len_list <- map(js_len_list, function(x) {return(c(x, rep(0, Nmax - length(x))))})

# The matrix of Javascript obfuscated code
js_obf_len_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  js_obf_len_list <- c(js_obf_len_list, list(train_websites_country$js_obf_len_bin))
}

js_obf_len_list <- map(js_obf_len_list, function(x) {return(c(x, rep(0, Nmax - length(x))))})

# The matrix of safety level of the URL, varying length of vector element
https_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  https_list <- c(https_list, list(train_websites_country$https_bin))
}
https_list <- map(https_list, function(x) {return(c(x, rep(0, Nmax - length(x))))})

# The matrix of safety level of the URL, varying length of vector element
whois_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  whois_list <- c(whois_list, list(train_websites_country$whois_bin))
}
whois_list <- map(whois_list, function(x) {return(c(x, rep(0, Nmax - length(x))))})

# The matrix of Javascript code, varying length of vector element
js_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  js_len_test_list <- c(js_len_test_list, list(test_websites_country$js_len_bin))
}
js_len_test_list <- map(js_len_test_list, function(x) {return(c(x, rep(0, Mmax - length(x))))})

# The matrix of Javascript code, varying length of vector element
js_obf_len_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  js_obf_len_test_list <- c(js_obf_len_test_list, list(test_websites_country$js_obf_len_bin))
}
js_obf_len_test_list <- map(js_obf_len_test_list, function(x) {return(c(x, rep(0, Mmax - length(x))))})

# The matrix of safety level of the URL, varying length of vector element
https_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  https_test_list <- c(https_test_list, list(test_websites_country$https_bin))
}
https_test_list <- map(https_test_list, function(x) {return(c(x, rep(0, Mmax - length(x))))})

# The matrix of safety level of the URL, varying length of vector element
whois_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  whois_test_list <- c(whois_test_list, list(test_websites_country$whois_bin))
}
whois_test_list <- map(whois_test_list, function(x) {return(c(x, rep(0, Mmax - length(x))))})

# The matrix of label of the URL (malicious/benign), varying length of vector element
label_list = list()
for (country in countries) {
  train_websites_country <- train_websites_top_5 %>%
    filter(geo_loc == country)
  label_list <- append(label_list, list(train_websites_country$label_bin))
}
label_list <- map(label_list, function(x) {return(c(x, rep(0, Nmax - length(x))))})

# The matrix of label of the URL (malicious/benign), varying length of vector element
label_test_list = list()
for (country in countries) {
  test_websites_country <- test_websites_top_5 %>%
    filter(geo_loc == country)
  label_test_list <- append(label_test_list, list(test_websites_country$label_bin))
}
label_test_list <- map(label_test_list, function(x) {return(c(x, rep(0, Mmax - length(x))))})

```

The Stan model code:
```{r, echo = TRUE, results = 'hide'}
"

"
```

```{r, echo=FALSE}
# Compiling the hierarchical Stan model
file_hierarchical <- file.path("model_hierarchical.stan")
model_hierarchical <- cmdstan_model(file_hierarchical)
model_hierarchical$compile(quiet = FALSE)
```
```{r, echo=FALSE}
# Compiling the pooled Stan model
file_pooled <- file.path("model_pooled.stan")
model_pooled <- cmdstan_model(file_pooled)
model_pooled$compile(quiet = FALSE)
```

The sampling running options
```{r, echo=FALSE}
stan_data_hierarchical <- list(
  Nmax = Nmax,
  Mmax = Mmax,
  K = K,
  N_list = N_list,
  M_list = M_list,
  js_len_list = as.matrix(do.call(rbind, js_len_list)),
  js_obf_len_list = as.matrix(do.call(rbind, js_obf_len_list)),
  https_list = as.matrix(do.call(rbind, https_list)),
  whois_list = as.matrix(do.call(rbind, whois_list)),
  js_len_pred_list = as.matrix(do.call(rbind, js_len_test_list)),
  js_obf_len_pred_list = as.matrix(do.call(rbind, js_obf_len_test_list)),
  https_pred_list = as.matrix(do.call(rbind, https_test_list)),
  whois_pred_list = as.matrix(do.call(rbind, whois_test_list)),
  label_list = as.matrix(do.call(rbind, label_list))
)

  hierarchical_sampling <- model_hierarchical$sample(data = stan_data_hierarchical, chains=4, iter_warmup = 2000, iter_sampling = 2000, refresh=0)
 

```

```{r, echo=FALSE}
stan_data_pooled <- list(
  Nmax = Nmax,
  Mmax = Mmax,
  K = K,
  N_list = N_list,
  M_list = M_list,
  js_len_list = as.matrix(do.call(rbind, js_len_list)),
  js_obf_len_list = as.matrix(do.call(rbind, js_obf_len_list)),
  https_list = as.matrix(do.call(rbind, https_list)),
  whois_list = as.matrix(do.call(rbind, whois_list)),
  js_len_pred_list = as.matrix(do.call(rbind, js_len_test_list)),
  js_obf_len_pred_list = as.matrix(do.call(rbind, js_obf_len_test_list)),
  https_pred_list = as.matrix(do.call(rbind, https_test_list)),
  whois_pred_list = as.matrix(do.call(rbind, whois_test_list)),
  label_list = as.matrix(do.call(rbind, label_list))
)

  pooled_sampling <- model_pooled$sample(data = stan_data_pooled, chains=4, iter_warmup = 2000, iter_sampling = 2000)
 

```

```{r}
hierarchical_sampling$output
```

```{r}
m <- hierarchical_sampling$draws("intercept", format="matrix")
m$
```
```{r}
# Set up the plotting grid
par(mfrow = c(3,5))

row_labels <- c("intercept", "HTTPS coefficient", "WHOIS coefficient")
row_names <- c("intercept","https_coeff","whois_coeff")

# Loop through the countries
for(i in 1:3){
    for (j in 1:5){
      # Create the subplot
      hist(hierarchical_sampling$draws(row_names[i]), main = countries[j], xlab=row_labels[i])
      # Add the country name to the top of the column
      mtext(countries[j], side = 3, line = 0.2, outer = TRUE)
    }
}

# Set up the plotting grid
par(mfrow = c(2,5))

row_labels <- c("JS length coefficient","JS obf. length coefficient")
row_names <- c("js_len_coeff","js_obf_len_coeff")

# Loop through the countries
for(i in 1:2){
    for (j in 1:5){
      # Create the subplot
      hist(hierarchical_sampling$draws(row_names[i]), main = countries[j], xlab=row_labels[i])
      # Add the country name to the top of the column
      mtext(countries[j], side = 3, line = 0.2, outer = TRUE)
    }
}
```

## Convergence diagnostics

MCMC convergence chains visualization
```{r, echo=FALSE}
plotConvergence <- function (draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  iters = length(chain1)
  indices <- 1:iters
  data <- data.frame(indices, chain1, chain2, 
                              chain3, chain4)
  
  ggplot(data, aes(x=indices)) +
    ggtitle(paste("hierarchical model - USA country\nFour MCMC of the",paramName,"\n",iters,"iterations, no warm-up")) +
    xlab("iteration") + 
    ylab(paramName) +
    theme(plot.title = element_text(hjust = 0.5), legend.position = "right") +
    geom_line(aes(y = chain1, color = "chain1")) + 
    geom_line(aes(y = chain2, color = "chain2")) +
    geom_line(aes(y = chain3, color = "chain3")) + 
    geom_line(aes(y = chain4, color = "chain4")) +
    scale_color_manual(name = "MCMC chains", values = c("chain1" = "red", "chain2" = "blue", "chain3" = "green", "chain4" = "black")) 
}

intercept_draws <- hierarchical_sampling[[countries[1]]]$draws("intercept", format = "matrix")
plotConvergence(intercept_draws, "intercept")

https_draws <- hierarchical_sampling[[countries[1]]]$draws("https_coeff", format = "matrix")
plotConvergence(https_draws, "https coefficient")

whois_draws <- hierarchical_sampling[[countries[1]]]$draws("whois_coeff", format = "matrix")
plotConvergence(whois_draws, "https coefficient")

js_len_draws <- hierarchical_sampling[[countries[1]]]$draws("js_len_coeff", format = "matrix")
plotConvergence(js_len_draws, "JS length coefficient")

js_obf_len_draws <- hierarchical_sampling[[countries[1]]]$draws("js_obf_len_coeff", format = "matrix")
plotConvergence(js_obf_len_draws, "JS obfuscated length coefficient")
```

Convergence analysis using $\hat{R}$-values
```{r}
# Rhat is in assignment 5
calculate_Rhat <- function(draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  
  sims <- matrix(c(chain1, chain2, chain3, chain4), ncol = 4)
  cat("\nThe Rhat value for",paramName,"after 2000 warm-up iterations is:",rhat_basic(x=sims, split=TRUE))

}

# ESS is in assignment 4
calculate_ESS <- function(draws, paramName){
  chain1 <- as.vector(draws[1:2000, 1])
  chain2 <- as.vector(draws[2001:4000, 1])
  chain3 <- as.vector(draws[4001:6000, 1])
  chain4 <- as.vector(draws[6001:8000, 1])
  
  sims <- matrix(c(chain1, chain2, chain3, chain4), ncol = 4)
  cat("\nThe Rhat value for",paramName,"after 2000 warm-up iterations is:",rhat_basic(x=sims, split=TRUE))

}

# You can also use summary() to calculate Rhat and ESS but it is kinda slow this time. We don't know why
# hierarchical_sampling[[countries[1]]]$summary()

intercept_draws <- hierarchical_sampling[[countries[1]]]$draws("intercept", format = "matrix")
calculate_Rhat(intercept_draws, "intercept")

https_draws <- hierarchical_sampling[[countries[1]]]$draws("https_coeff", format = "matrix")
calculate_Rhat(https_draws, "https coefficient")

whois_draws <- hierarchical_sampling[[countries[1]]]$draws("whois_coeff", format = "matrix")
calculate_Rhat(whois_draws, "whois coefficient")

js_len_draws <- hierarchical_sampling[[countries[1]]]$draws("js_len_coeff", format = "matrix")
calculate_Rhat(js_len_draws, "JS length coefficient")

js_obf_len_draws <- hierarchical_sampling[[countries[1]]]$draws("js_obf_len_coeff", format = "matrix")
calculate_Rhat(js_obf_len_draws, "JS obfuscated length coefficient")

```
HMC specific convergence diagnostics
```{r, echo=FALSE}
for (i in 1:K){
  cat("Country",countries[i],"\n\n")
  print(hierarchical_sampling[[countries[i]]]$diagnostic_summary())
}
```
Rhat values and effective sample size
```{r}
hierarchical_sampling[[countries[1]]]$summary(c("intercept", "https_coeff","whois_coeff","js_len_coeff","js_obf_len_coeff"))[, c("variable", "rhat", "ess_bulk", "ess_tail")]

```
## Posterior predictive checks
```{r}
metricsName <- c("Accuracy", "Prevalence", "Sensitivity", "Specificity", "Precision", "Recall", "F1")
metricsSummary <- data.frame(Metrics=metricsName)
metricsSummary$Accuracy <- NULL
metricsSummary$Prevalence <- NULL
metricsSummary$Precision <- NULL
metricsSummary$Recall <- NULL
metricsSummary$Sensitivity <- NULL
metricsSummary$Specificity <- NULL
metricsSummary$F1 <- NULL

sumTP <- 0
sumTN <- 0
sumFP <- 0
sumFN <- 0
listTrue <- c()
listPred <- c()
for (i in 1:K){
  draws <- hierarchical_sampling[[countries[i]]]$draws("label_train_pred", format = "matrix")
  confusion_matrix <- table(as.vector(draws[8000,]), label_list[[i]])
  listTrue <- c(listTrue, label_list[[i]])
  listPred <- c(listPred, as.vector(draws[8000,]))

  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <-confusion_matrix[1,2]
  FN <-confusion_matrix[2,1]
  sumTP <- sumTP + TP
  sumTN <- sumTN + TN
  sumFP <- sumFP + FP
  sumFN <- sumFN + FN
  accuracy <- (TP+TN)/(TP+FP+FN+TN) 
  prevalence <- (TP+FN)/(TP+FP+FN+TN) 
  sensitivity <- TP/(TP+FN)
  specificity <- FN/(FN+FP)
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN)
  f1 <-  2*(precision*recall)/(precision+recall)
  metricsSummary[,countries[i]] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
}
accuracy <- (sumTP+sumTN)/(sumTP+sumFP+sumFN+sumTN) 
prevalence <- (sumTP+sumFN)/(sumTP+sumFP+sumFN+sumTN) 
sensitivity <- sumTP/(sumTP+sumFN)
specificity <- sumFN/(sumFN+sumFP)
precision <- sumTP/(sumTP+sumFP)
recall <- sumTP/(sumTP+sumFN)
f1 <-  2*(precision*recall)/(precision+recall)
metricsSummary[,"All countries"] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
metricsSummary
```

```{r, warning = FALSE}
confusion_matrix <- tibble("actual" = listTrue,
                     "prediction" = listPred)


basic_table <- table(confusion_matrix)
cfm <- as_tibble(basic_table)
plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n", palette = "Oranges")
```

## Predictive performance assessment

```{r}
metricsName <- c("Accuracy", "Prevalence", "Sensitivity", "Specificity", "Precision", "Recall", "F1")
metricsSummary <- data.frame(Metrics=metricsName)
metricsSummary$Accuracy <- NULL
metricsSummary$Prevalence <- NULL
metricsSummary$Precision <- NULL
metricsSummary$Recall <- NULL
metricsSummary$Sensitivity <- NULL
metricsSummary$Specificity <- NULL
metricsSummary$F1 <- NULL

sumTP <- 0
sumTN <- 0
sumFP <- 0
sumFN <- 0
listTrue <- c()
listPred <- c()
for (i in 1:K){
  draws <- hierarchical_sampling[[countries[i]]]$draws("label_test_pred", format = "matrix")
  confusion_matrix <- table(as.vector(draws[8000,]), label_test_list[[i]])
  listTrue <- c(listTrue, label_test_list[[i]])
  listPred <- c(listPred, as.vector(draws[8000,]))

  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <-confusion_matrix[1,2]
  FN <-confusion_matrix[2,1]
  sumTP <- sumTP + TP
  sumTN <- sumTN + TN
  sumFP <- sumFP + FP
  sumFN <- sumFN + FN
  accuracy <- (TP+TN)/(TP+FP+FN+TN) 
  prevalence <- (TP+FN)/(TP+FP+FN+TN) 
  sensitivity <- TP/(TP+FN)
  specificity <- FN/(FN+FP)
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN)
  f1 <-  2*(precision*recall)/(precision+recall)
  metricsSummary[,countries[i]] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
}
accuracy <- (sumTP+sumTN)/(sumTP+sumFP+sumFN+sumTN) 
prevalence <- (sumTP+sumFN)/(sumTP+sumFP+sumFN+sumTN) 
sensitivity <- sumTP/(sumTP+sumFN)
specificity <- sumFN/(sumFN+sumFP)
precision <- sumTP/(sumTP+sumFP)
recall <- sumTP/(sumTP+sumFN)
f1 <-  2*(precision*recall)/(precision+recall)
metricsSummary[,"All countries"] <- c(accuracy, prevalence, sensitivity, specificity, precision,recall,f1)
metricsSummary
```

```{r, warning = FALSE}
confusion_matrix <- tibble("actual" = listTrue,
                     "prediction" = listPred)


basic_table <- table(confusion_matrix)
cfm <- as_tibble(basic_table)
plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n", palette = "Oranges")
```


## Prior sensitivity analysis




# Model comparison



# Discussion

## Existing issues

## Potential improvements

# Conclusion




# Reflection


# References


